{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014c8d3f",
   "metadata": {},
   "source": [
    "ğŸ“Œ RAG (Retrieval-Augmented Generation) \n",
    "\n",
    "What is RAG & Why We Use It\n",
    "ğŸ“Œ Definition\n",
    "RAG = A method where an LLM retrieves relevant documents from a knowledge base (vector database) and uses them as context to generate an accurate response.\n",
    "\n",
    "ğŸ“Œ Why we use it\n",
    "\n",
    "LLMs (like GPT) are trained on static data â†’ They donâ€™t know new company policies, updated laws, or private datasets.\n",
    "\n",
    "RAG lets LLMs pull fresh, domain-specific knowledge at runtime â€” without retraining.\n",
    "\n",
    "ğŸ“Œ Analogy\n",
    "\n",
    "LLM without RAG â†’ A student answering from memory only (can forget or hallucinate).\n",
    "\n",
    "LLM with RAG â†’ A student answering with an open book (checks the book before answering).\n",
    "\n",
    "\n",
    "2ï¸âƒ£ How RAG Works (Step-by-Step Architecture)\n",
    "RAG = Retrieval + Generation.\n",
    "\n",
    "Step 1: Retrieval\n",
    "User asks a question (Query).\n",
    "\n",
    "Query is converted into vector embedding.\n",
    "\n",
    "Vector Database (FAISS, Pinecone, Chroma) searches for similar documents.\n",
    "\n",
    "Returns top k relevant chunks.\n",
    "\n",
    "Step 2: Generation\n",
    "Retrieved chunks are added as context to LLM prompt.\n",
    "\n",
    "LLM reads query + context.\n",
    "\n",
    "Generates final answer.\n",
    "\n",
    "\n",
    "flow diagram -\n",
    "User Query â†’ Embed Query\n",
    "   â†“\n",
    "Retriever (Search in Vector DB)\n",
    "   â†“\n",
    "Top Matching Chunks\n",
    "   â†“\n",
    "LLM (Generates answer using chunks)\n",
    "   â†“\n",
    "Response\n",
    "\n",
    "\n",
    "\n",
    "3ï¸âƒ£ Key Components of RAG\n",
    "A) Embeddings\n",
    "Convert text into numerical vectors.\n",
    "\n",
    "Similar meanings â†’ Vectors close together.\n",
    "\n",
    "Example: â€œCEO of Appleâ€ and â€œTim Cookâ€ have close vectors.\n",
    "\n",
    "ğŸ“Œ Common Embedding Models\n",
    "\n",
    "OpenAI embeddings (text-embedding-ada-002)\n",
    "\n",
    "HuggingFace Sentence Transformers\n",
    "\n",
    "B) Vector Database\n",
    "Stores embeddings for documents.\n",
    "\n",
    "Finds most similar documents for a query.\n",
    "\n",
    "ğŸ“Œ Popular Vector Databases\n",
    "\n",
    "FAISS (local, small-medium projects)\n",
    "\n",
    "Pinecone (cloud, scalable)\n",
    "\n",
    "Chroma (simple, open-source)\n",
    "\n",
    "Weaviate, Milvus (enterprise scale)\n",
    "\n",
    "C) Retriever\n",
    "Pulls top matching chunks from Vector DB.\n",
    "\n",
    "D) LLM\n",
    "Takes query + retrieved context.\n",
    "\n",
    "Generates final answer.\n",
    "\n",
    "Example: GPT, LLaMA, Falcon.\n",
    "\n",
    "4ï¸âƒ£ Real-World RAG Applications\n",
    "A) HR Chatbot\n",
    "Knowledge base: HR policies.\n",
    "\n",
    "Use: Employees ask about leave policies, insurance, salary.\n",
    "\n",
    "B) Legal Document Assistant\n",
    "Knowledge base: Contracts, laws.\n",
    "\n",
    "Use: Lawyers search for penalty clauses.\n",
    "\n",
    "C) Research Assistant\n",
    "Knowledge base: Academic papers.\n",
    "\n",
    "Use: Summarizes findings for scientists.\n",
    "\n",
    "D) Customer Support Bot\n",
    "Knowledge base: Product manuals, FAQs.\n",
    "\n",
    "Use: Answers troubleshooting queries.\n",
    "\n",
    "E) Financial Data Assistant\n",
    "Knowledge base: Annual reports, balance sheets.\n",
    "\n",
    "Use: Answers investment-related questions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
