{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014c8d3f",
   "metadata": {},
   "source": [
    "📌 RAG (Retrieval-Augmented Generation) \n",
    "\n",
    "What is RAG & Why We Use It\n",
    "📌 Definition\n",
    "RAG = A method where an LLM retrieves relevant documents from a knowledge base (vector database) and uses them as context to generate an accurate response.\n",
    "\n",
    "📌 Why we use it\n",
    "\n",
    "LLMs (like GPT) are trained on static data → They don’t know new company policies, updated laws, or private datasets.\n",
    "\n",
    "RAG lets LLMs pull fresh, domain-specific knowledge at runtime — without retraining.\n",
    "\n",
    "📌 Analogy\n",
    "\n",
    "LLM without RAG → A student answering from memory only (can forget or hallucinate).\n",
    "\n",
    "LLM with RAG → A student answering with an open book (checks the book before answering).\n",
    "\n",
    "\n",
    "2️⃣ How RAG Works (Step-by-Step Architecture)\n",
    "RAG = Retrieval + Generation.\n",
    "\n",
    "Step 1: Retrieval\n",
    "User asks a question (Query).\n",
    "\n",
    "Query is converted into vector embedding.\n",
    "\n",
    "Vector Database (FAISS, Pinecone, Chroma) searches for similar documents.\n",
    "\n",
    "Returns top k relevant chunks.\n",
    "\n",
    "Step 2: Generation\n",
    "Retrieved chunks are added as context to LLM prompt.\n",
    "\n",
    "LLM reads query + context.\n",
    "\n",
    "Generates final answer.\n",
    "\n",
    "\n",
    "flow diagram -\n",
    "User Query → Embed Query\n",
    "   ↓\n",
    "Retriever (Search in Vector DB)\n",
    "   ↓\n",
    "Top Matching Chunks\n",
    "   ↓\n",
    "LLM (Generates answer using chunks)\n",
    "   ↓\n",
    "Response\n",
    "\n",
    "\n",
    "\n",
    "3️⃣ Key Components of RAG\n",
    "A) Embeddings\n",
    "Convert text into numerical vectors.\n",
    "\n",
    "Similar meanings → Vectors close together.\n",
    "\n",
    "Example: “CEO of Apple” and “Tim Cook” have close vectors.\n",
    "\n",
    "📌 Common Embedding Models\n",
    "\n",
    "OpenAI embeddings (text-embedding-ada-002)\n",
    "\n",
    "HuggingFace Sentence Transformers\n",
    "\n",
    "B) Vector Database\n",
    "Stores embeddings for documents.\n",
    "\n",
    "Finds most similar documents for a query.\n",
    "\n",
    "📌 Popular Vector Databases\n",
    "\n",
    "FAISS (local, small-medium projects)\n",
    "\n",
    "Pinecone (cloud, scalable)\n",
    "\n",
    "Chroma (simple, open-source)\n",
    "\n",
    "Weaviate, Milvus (enterprise scale)\n",
    "\n",
    "C) Retriever\n",
    "Pulls top matching chunks from Vector DB.\n",
    "\n",
    "D) LLM\n",
    "Takes query + retrieved context.\n",
    "\n",
    "Generates final answer.\n",
    "\n",
    "Example: GPT, LLaMA, Falcon.\n",
    "\n",
    "4️⃣ Real-World RAG Applications\n",
    "A) HR Chatbot\n",
    "Knowledge base: HR policies.\n",
    "\n",
    "Use: Employees ask about leave policies, insurance, salary.\n",
    "\n",
    "B) Legal Document Assistant\n",
    "Knowledge base: Contracts, laws.\n",
    "\n",
    "Use: Lawyers search for penalty clauses.\n",
    "\n",
    "C) Research Assistant\n",
    "Knowledge base: Academic papers.\n",
    "\n",
    "Use: Summarizes findings for scientists.\n",
    "\n",
    "D) Customer Support Bot\n",
    "Knowledge base: Product manuals, FAQs.\n",
    "\n",
    "Use: Answers troubleshooting queries.\n",
    "\n",
    "E) Financial Data Assistant\n",
    "Knowledge base: Annual reports, balance sheets.\n",
    "\n",
    "Use: Answers investment-related questions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
