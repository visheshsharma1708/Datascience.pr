{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1430e908",
   "metadata": {},
   "source": [
    "## Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894fc926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\svish\\Desktop\\jupyter-notebook\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[[0.42231882 0.15536243 0.42231882]\n",
      "  [0.01587624 0.86681336 0.11731043]\n",
      "  [0.15536243 0.42231882 0.42231882]]]\n",
      "Output:\n",
      " [[[0.84463763 0.7330437  0.84463763 0.7330437 ]\n",
      "  [0.13318667 1.8509371  0.13318667 1.8509371 ]\n",
      "  [0.57768124 1.2669564  0.57768124 1.2669564 ]]]\n"
     ]
    }
   ],
   "source": [
    "# self attention \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Sample Q, K, V (batch=1, sequence=3 tokens, embedding=4)\n",
    "Q = tf.constant([[[1.0, 0.0, 1.0, 0.0],\n",
    "                  [0.0, 2.0, 0.0, 2.0],\n",
    "                  [1.0, 1.0, 1.0, 1.0]]])  \n",
    "\n",
    "K = Q  # For self-attention, Q=K=V\n",
    "V = Q\n",
    "\n",
    "# Step 1: Scores\n",
    "scores = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "# Step 2: Scale\n",
    "d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "scaled_scores = scores / tf.math.sqrt(d_k)\n",
    "\n",
    "# Step 3: Softmax\n",
    "weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "\n",
    "# Step 4: Weighted sum\n",
    "output = tf.matmul(weights, V)\n",
    "\n",
    "print(\"Attention Weights:\\n\", weights.numpy())\n",
    "print(\"Output:\\n\", output.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a75b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 5, 64)\n",
      "Attention Weights shape: (1, 4, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "# multi head attention\n",
    "from keras.layers import MultiHeadAttention\n",
    "\n",
    "mha = MultiHeadAttention(num_heads=4, key_dim=64)\n",
    "query = tf.random.normal((1, 5, 64))  # batch=1, sequence=5, embedding=64\n",
    "key = query\n",
    "value = query\n",
    "\n",
    "output, weights = mha(query, key, value, return_attention_scores=True)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention Weights shape:\", weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705c121",
   "metadata": {},
   "source": [
    "## position encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee9b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding shape: (1, 50, 512)\n",
      "tf.Tensor(\n",
      "[[0.         1.         0.         1.         0.         1.\n",
      "  0.         1.        ]\n",
      " [0.84147096 0.5403023  0.8218562  0.569695   0.8019618  0.59737533\n",
      "  0.7818871  0.62342006]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def positional_encoding(position: int, d_model: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Generates sinusoidal positional encoding for Transformers.\n",
    "\n",
    "    Args:\n",
    "        position (int): Maximum sequence length.\n",
    "        d_model (int): Embedding dimension size.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Positional encoding tensor of shape (1, position, d_model).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create position and dimension indices\n",
    "    positions = np.arange(position)[:, np.newaxis]  # Shape: (position, 1)\n",
    "    dims = np.arange(d_model)[np.newaxis, :]        # Shape: (1, d_model)\n",
    "    \n",
    "    # Compute angle rates for each position & dimension\n",
    "    angle_rates = positions / np.power(10000, (2 * (dims // 2)) / np.float32(d_model))\n",
    "    \n",
    "    # Apply sin to even indices (2i) and cos to odd indices (2i+1)\n",
    "    angle_rates[:, 0::2] = np.sin(angle_rates[:, 0::2])  # Even indices\n",
    "    angle_rates[:, 1::2] = np.cos(angle_rates[:, 1::2])  # Odd indices\n",
    "    \n",
    "    # Add batch dimension (1, position, d_model)\n",
    "    pos_encoding = angle_rates[np.newaxis, ...]\n",
    "    \n",
    "    return tf.constant(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Example: Generate positional encoding\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "print(f\"Positional Encoding shape: {pos_encoding.shape}\")\n",
    "print(pos_encoding[0, :2, :8])  # Show first 2 positions & first 8 dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a43e2a",
   "metadata": {},
   "source": [
    "### customer transformer block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd657947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 5)\n",
      "Output shape: (1, 5, 32)\n",
      "Output tensor:\n",
      " tf.Tensor(\n",
      "[[[-1.2139783   0.7901611  -0.07223275  1.7702596  -1.1192064\n",
      "    0.33365262 -1.3610643   1.1406313  -0.49382725  1.993513\n",
      "   -1.0768187   0.9004701  -1.595818    1.1921027   0.24211189\n",
      "    0.47187516 -0.02194184  0.49221906 -0.4757451   0.33982074\n",
      "    0.1217721   0.8884668  -1.0852524   0.54455274 -1.8132877\n",
      "    0.77975506 -1.3674005   0.9389294  -0.21363592  0.4453123\n",
      "   -1.4323765  -0.04302042]\n",
      "  [-0.01251765 -0.4043251   0.9231734   1.5236789  -0.93805635\n",
      "    0.6337972  -1.073817    1.1407843  -0.6726321   1.8222383\n",
      "   -1.0959375   1.0190835  -1.893938    1.1799638   0.29763266\n",
      "    0.39238638 -0.68082273  0.80426776 -0.8170506   0.31224462\n",
      "   -0.22182408  1.0936482  -1.2231823   0.8265066  -1.6600093\n",
      "    0.6380167  -1.4227729   1.0076244  -0.31770766  0.44771788\n",
      "   -1.3445395  -0.28363237]\n",
      "  [-0.01247142 -2.3561685   1.6658335   0.96436787 -0.51659286\n",
      "    0.47481412 -0.9704659   0.41194165 -0.26510707  1.5963167\n",
      "   -0.83291215  0.90353197 -1.8051639   1.1849289   0.6492589\n",
      "    0.53210044 -0.84526604  0.7381773  -0.55569124  0.5367129\n",
      "   -0.25626534  1.1091264  -0.8760236   0.8542237  -1.2786791\n",
      "    1.0018005  -1.0935596   0.8275025  -0.73112595  0.3662142\n",
      "   -1.3416556  -0.07970273]\n",
      "  [-1.2691791  -3.4006116   1.6688571   0.50037825  0.0207764\n",
      "   -0.14761987 -0.66008    -0.11672471  0.3466252   1.2829287\n",
      "   -0.6995761   0.6554945  -1.0300606   1.1487994   0.75289786\n",
      "    0.7390819  -0.66252446  0.30434465 -0.41570812  0.5449303\n",
      "    0.04667147  0.78669006 -0.5398185   0.774701   -1.1874359\n",
      "    1.2412798  -0.71679133  0.89120835 -0.5716746   0.42715663\n",
      "   -1.2522993   0.53728265]\n",
      "  [-2.7981453  -2.357792    1.076889    0.03240367  0.2552231\n",
      "   -0.69786614 -0.63958555 -0.48694068  0.88622475  1.0613625\n",
      "   -0.8845938   0.6068547  -0.48777384  1.2831049   0.9270057\n",
      "    0.7667122  -0.17916036 -0.27075362 -0.6748314   0.5394963\n",
      "    0.4040236   0.58218956 -0.31166714  0.921098   -1.3099478\n",
      "    1.5199401  -0.60945565  0.9189413  -0.3065682   0.47704422\n",
      "   -1.1129559   0.8695232 ]]], shape=(1, 5, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LayerNormalization, Dense, Dropout, Embedding\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "# Positional Encoding Function\n",
    "def positional_encoding(position, d_model):\n",
    "    positions = np.arange(position)[:, np.newaxis]\n",
    "    dims = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = positions / np.power(10000, (2 * (dims // 2)) / np.float32(d_model))\n",
    "    angle_rates[:, 0::2] = np.sin(angle_rates[:, 0::2])  # Even indices\n",
    "    angle_rates[:, 1::2] = np.cos(angle_rates[:, 1::2])  # Odd indices\n",
    "    pos_encoding = angle_rates[np.newaxis, ...]  # Shape: (1, position, d_model)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # Residual + Norm\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)  # Residual + Norm\n",
    "\n",
    "# Model with Embedding + Positional Encoding + Transformer Block\n",
    "class SimpleTransformer(Model):\n",
    "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = positional_encoding(max_len, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # Word embeddings\n",
    "        x += self.pos_encoding[:, :seq_len, :]  # Add positional encoding\n",
    "        x = self.transformer_block(x)\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 1000\n",
    "max_len = 20\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "\n",
    "# Create Model\n",
    "model = SimpleTransformer(vocab_size, max_len, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Dummy Input\n",
    "dummy_input = tf.constant([[1, 5, 23, 45, 67]])  # Shape: (batch=1, sequence=5)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output tensor:\\n\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
